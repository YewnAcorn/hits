Carter Gunderson
7 week workshop thursday
worked with Jeff and Joe

helped eachother understand the formulas
when one term or another was confusing

I have all the equations written out now
in a way I understand, the next step is
just to plug in the values from the example

## update, values plugged in and calculated

T1
 |
 0
 |
 A
 |
 -1
 |
 B
 | 
 1 
 |
 C
 |
 1
 |
 T2


T1 - (0) - [A] - (-1) - [B] - (1) - [C] - (1) - T2


If the values of the states A, B, and C are 2, 1, and 3 respectively,
update the value of the state B in each of the 3 frameworks 
(TD(0), MC, and DP), given the following trace for MC 
and that the probability of moving right or left is 0.5 in each state.

B -> A -> B -> C -> T

A=2
B=1
C=3

## Dynamic Programming:

policy evaluation (fixed policy)
V(S[t]) = wavg(R[t+1] + gamma * V(S[t+1])

probability is 0.5 and there are 2 options
option 1 go left: R[t+1] = -1 V(S[t+1] = A = 2
option 2 go right: R[t+1] = 1 V(S[t+1] = C = 3
B = 0.5(-1 + 0.9 * 2) + 0.5( 1 + 0.9 * 3)
B = 0.5(0.8) + 0.5(3.7)
B = 0.4 + 1.85
B = 2.25  # and then does it need to be divided by 2 because there's 2 options, left and right?
B = 1.125

policy evaluation (optimal policy)
Vstar(s) = max(R[t+1] + gamma*v(s[t+1])
max would be to the right because it has a higher
result 0.4 < 1.85
B = 1.85

policy improvement
newpolicy = argmax(wavg(R[t+1] + gamma * v(s[t+1])))

not sure what goes here for B, is it different than before?
B = 1.85

## TD(0):

V(S[t]) = V(S[t]) + alpha( R[t+1] + gamma * V(S[t+1]) - V(S[t]) ) 

B = 1 + 0.1 ( -1 + (0.9 * 2) - 1)
B = 0.98

B = 0.98 + 0.1 ( 1 + (0.9 * 3 -1 ) 
B = 3.25



## Monte Carlo:

policy evaluation (on-policy)
G[t](s) = r[t+1] + gamma * r[t+2] + gamma^[t+i] * r[t+i] +...
V(s) = V(s) + alpha * (G[t](s) - V(s))

r[t+1] = -1   r[t+2] = -1   r[t+3] = 1    r[t+4] = 1 
G[0](B) = -1 + 0.9*-1 + (0.9^2)*1 + (0.9^3)*1
G[0](B) = -0.361

V(B) starts as 1

V(B) = 1 + 0.1 * (-0.361 - 1)
V(B) = 0.8639

for off-policy, you need to weight the error g[t] -v by
the importance value

policy improvement
epsilon greedy. When would this happen? At the end of
an episode?


## n-step TD

policy evaluation
choose an action according to policy
G[t](s) = r[t+1] + ... + gamma^(n-1) * r[t+n] + gamma^n * v(s[t+n]) 
v[pi](s) = v[pi](s) + alpha * g[t](s) - v[pi](s)


for a 2 step TD
r[t+1] = -1
r[t+2] = -1
v(s[t+2]) = B = 1 
G[0](B) = -1 + 0.9*-1 + (0.9^2)*1
        = -1.9 + 0.81
        = -1.09






